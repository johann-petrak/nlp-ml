class SequenceProcessor(Processor):
    """
    For processing items that come from a serial source or go to a serial source.
    Optionally can send data back to a serial destination.
    """

    def __init__(self, source, nprocesses=1, pipeline=None,
                 destination=None, use_destination_tuple=False, maxsize_iqueue=10,
                 maxsize_oqueue=10, runtimeargs={}, maxerrors=0):
        """
        Process some serial access source and optionally send the processed items to
        a serial destination.
        :param source: anything that can be iterated over or a SerialSource object
        (an instance of collections.Iterator). If the object has a "close" attribute that
        is callable, it is closed when the iterator is exhausted.
        :param nprocesses: number of processes to run in parallel. If 1, no multiprocessing code is used. If 0,
        uses as many processes as there are CPUs. If < 0, use as many processes as there are cpus, but at most
        abs(nprocesses).
        :param pipeline: the processing pipeline, a single Pr or a list of Prs
        :param destination: a SerialDestination object or anything that implements write and optionally close.
        :param use_destination_tuple: if True, send the tuple (id, item) to the write function of the destination
        instead of just item.
        :param maxsize_iqueue: the maximum number of items to put into the input queue before locking
        :param maxsize_oqueue: the maximum number of items to put into the output queue before locking
        :param runtimeargs: a dictionary of kwargs to add to the kwargs passed on to the Prs
        :param maxerrors: maximum number of continuable errors to allow per process before everything gets stopped.
        Note: errors in the reader or writer always terminate the process.
        """
        import collections
        if isinstance(source, collections.Iterator) or isinstance(source, collections.Iterable):
            self.source = source
        else:
            raise Exception("Source must be a SerialSource or any Iterator or Iterable")
        if nprocesses > 0:
            self.nprocesses = nprocesses
        elif nprocesses == 0:
            self.nprocesses = multiprocessing.cpu_count()
        else:
            self.nprocesses = min(multiprocessing.cpu_count(), abs(nprocesses))
        # if we use multiprocessing, check if the pipeline can be pickled!
        if self.nprocesses != 1:
            import pickle
            try:
                tmp = pickle.dumps(pipeline)
            except Exception as ex:
                raise Exception("Pipeline cannot be pickled, cannot use multiprocessing, error: {}".format(ex))
        self.pipeline = pipeline
        if hasattr(destination, "write") and callable(destination.write):
            pass
        else:
            raise Exception("Destination must be a SerialDestination instance or implement write(item) and optional close()")
        self.destination = destination
        self.maxsize_iqeueue = maxsize_iqueue
        self.maxsize_oqeueue = maxsize_oqueue
        self.runtimeargs = runtimeargs
        self.use_destination_tuple = use_destination_tuple
        self.maxerrors = maxerrors

    def run(self):
        """
        Actually runs the pipeline over all the data. Returns a tuple of number of items processed
        in total, and number of items that had some error.
        :return: a tuple, total number of items, items with error, if reader had errors, if writer had errors
        """
        logger = logging.getLogger(__name__+".SequenceProcessor.run")
        n_total = 0
        n_nok = 0
        if self.nprocesses == 1:
            # just run everything directly in here, no multiprocessing
            # NOTE: if the source throws an error, this will bubble up and terminate the method so we do not
            # need to trap the error here!
            for id, item in enumerate(self.source):
                n_total += 1
                haveerror = False
                try:
                    if self.pipeline is not None:
                        item = run_pipeline_on(self.pipeline, item, id=id, pid=1)
                except Exception as ex:
                    logger.error("Error processing item {}: {}".format(id, ex))
                    n_nok += 1
                    if n_nok > self.maxerrors:
                        raise Exception("Maximum number of errors ({}) reached, aborting".format(n_nok))
                    haveerror = True
                if not haveerror and self.destination:
                    try:
                        if self.use_destination_tuple:
                            self.destination.write((id, item))
                        else:
                            self.destination.write(item)
                    except Exception as ex:
                        raise Exception("Error trying to write to the destination: {}".format(ex))
            # the reader and writer errors are thrown earlier, so if we arrive here, there were none
            return n_total, n_nok, False, False
        else:
            # ok, do the actual multiprocessing
            # first, check if the pipeline contains any Pr which is single process only
            if not ProcessingResource.supports_multiprocessing(self.pipeline):
                raise Exception("Cannot run multiprocessing, pipeline contains single processing PR")
            # shared value to indicate that we need to abort the workers if set to something other than zero
            abortflag = Value('i', 0)
            # set up a pool for the workers
            pool = Pool(self.nprocesses)
            kw = self.runtimeargs
            kw["nprocesses"] = self.nprocesses
            kw.update(self.runtimeargs)
            kw["abortflag"] = abortflag
            kw["maxerrors"] = self.maxerrors
            rets = []
            mgr = multiprocessing.Manager()
            input_queue = mgr.Queue(maxsize=self.maxsize_iqeueue)
            reader_pool = Pool(1)
            logger.debug("Running reader pool apply async")
            reader_pool_ret = pool.apply_async(source_reader, [input_queue, self.source], kw)
            output_queue = None
            if self.destination is not None:
                output_queue = mgr.Queue(maxsize=self.maxsize_oqeueue)
            pipeline_runner = PipelineRunnerSeq(input_queue, output_queue)
            for i in range(self.nprocesses):
                kw["pid"] = i
                tmpkw = kw.copy()
                logger.debug("Running worker pool process {} apply async".format(i))
                ret = pool.apply_async(pipeline_runner, (self.pipeline,), tmpkw)
                rets.append(ret)
            writer_error = False
            # handle writing the results to the destination in our own process here, that way, in-memory
            # destinations are created in our own process!
            if self.destination is not None:
                logger.debug("Calling destination writer with {} {} {} {}".format(output_queue, self.destination, self.nprocesses, kw))
                writer_error = False
                # we trap writer errors so we can try to wait for and finish the other processes!
                try:
                    destination_writer(output_queue, self.destination, **kw)
                except Exception as ex:
                    writer_error = True
                    abortflag.value = 1
                    logger.error("Encountered writer error: {}".format(ex))
            # now wait for the processes and pool to finish
            pool.close()
            pool.join()
            reader_pool.close()
            reader_pool.join()
            reader_error = reader_pool_ret.get()
            for r in rets:
                rval = r.get()
                n_total += rval[0]
                n_nok += rval[1]
            return n_total, n_nok, reader_error, writer_error
