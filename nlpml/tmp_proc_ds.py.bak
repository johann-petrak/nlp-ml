class DatasetProcessor(Processor):
    """
    For processing items from a Dataset, where accessing each item is independent
    of any other item. Each item can optionally be sent to a serial destination.
    """

    def __init__(self, dataset, destination=None, nprocesses=1, pipeline=None,
                 use_destination_tuple=False, maxsize_oqueue=10, runtimeargs={}):
        """
        Process a dataset with the pipeline. Note that the dataset could already wrap a ProcessDataset
        with its own pipeline. The pipeline specified here can be left as None to not do any
        processing at all.
        :param nprocesses: number of processes to run in parallel. If 1, no multiprocessing code is used. If 0,
        uses as many processes as there are CPUs. If < 0, use as many processes as there are cpus, but at most
        abs(nprocesses).
        :param pipeline: the processing pipeline, a single Pr or a list of Prs
        :param destination: a SerialDestination object or anything that implements write and optionally close.
        :param use_destination_tuple: if True, send the tuple (id, item) to the write function of the destination
        instead of just item.
        :param maxsize_oqueue: the maximum number of items to put into the output queue before locking
        :param runtimeargs: a dictionary of kwargs to add to the kwargs passed on to the Prs
        """
        # logging.getLogger(__name__).setLevel(logging.DEBUG)
        self.dataset = dataset
        if nprocesses > 0:
            self.nprocesses = nprocesses
        elif nprocesses == 0:
            self.nprocesses = multiprocessing.cpu_count()
        else:
            self.nprocesses = min(multiprocessing.cpu_count(), abs(nprocesses))
        # if we use multiprocessing, check if the pipeline can be pickled!
        if pipeline is not None and self.nprocesses != 1:
            import pickle
            try:
                tmp = pickle.dumps(pipeline)
            except Exception as ex:
                raise Exception("Pipeline cannot be pickled, cannot use multiprocessing, error: {}".format(ex))
        self.pipeline = pipeline
        if hasattr(destination, "write") and callable(destination.write):
            pass
        else:
            raise Exception("Destination must be a SerialDestination instance or implement write(item) and optional close()")
        self.destination = destination
        self.maxsize_oqeueue = maxsize_oqueue
        self.runtimeargs = runtimeargs
        self.use_destination_tuple = use_destination_tuple

    def run(self):
        """
        Actually runs the pipeline over the dataset. Returns a tuple of number of items processed
        in total, and number of items that had some error.
        :return: a tuple, total number of items, items with error, if writer had errors (or None if no destination)
        """
        logger = logging.getLogger(__name__+".DatasetProcessor.run")
        n_total = 0
        n_nok = 0
        if self.nprocesses == 1:
            # just run everything directly in here, no multiprocessing
            for id, item in enumerate(self.dataset):
                n_total += 1
                try:
                    if self.pipeline is not None:
                        item = run_pipeline_on(self.pipeline, item, id=id, pid=1)
                except Exception as ex:
                    logger.error("Error processing item {}: {}".format(id, ex))
                    n_nok += 1
                # now if there is a destination, pass it on to there
                if self.destination:
                    # TODO: catch writer error!
                    if self.use_destination_tuple:
                        self.destination.write((id, item))
                    else:
                        self.destination.write(item)
            # TODO: instead of the for loop, use something that allows to trap reader error
            return n_total, n_nok, False
        else:
            # ok, do the actual multiprocessing
            # first, check if the pipeline contains any Pr which is single process only
            if not ProcessingResource.supports_multiprocessing(self.pipeline):
                raise Exception("Cannot run multiprocessing, pipeline contains single processing PR")
            # first set up a pool for the workers
            pool = Pool(self.nprocesses)
            kw = self.runtimeargs
            kw["nprocesses"] = self.nprocesses
            kw.update(self.runtimeargs)
            rets = []
            mgr = multiprocessing.Manager()
            if self.destination is not None:
                output_queue = mgr.Queue(maxsize=self.maxsize_oqeueue)
            pipeline_runner = PipelineRunnerDataset(self.dataset, output_queue)
            for i in range(self.nprocesses):
                # logger.info("Starting process {}".format(i))
                kw["pid"] = i
                tmpkw = kw.copy()
                logger.debug("Running worker pool process {} apply async".format(i))
                ret = pool.apply_async(pipeline_runner, (self.pipeline,), tmpkw)
                rets.append(ret)
            writer_error = False
            if self.destination is not None:
                logger.debug("Calling destination writer with {} {} {} {}".format(output_queue, self.destination, self.nprocesses, kw))
                writer_error = destination_writer(output_queue, self.destination, **kw)
            pool.close()
            pool.join()
            for r in rets:
                rval = r.get()
                n_total += rval[0]
                n_nok += rval[1]
            return n_total, n_nok, writer_error
